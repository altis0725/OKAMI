main_crew:
  name: "OKAMI Main Crew"
  description: "Versatile AI crew that adapts to various tasks with intelligent task management"
  
  process: hierarchical  # Manager intelligently assigns tasks
  
  # Use custom manager agent instead of default LLM manager
  manager_agent: manager_agent
  # manager_llm: true

  # Planning configuration
  planning: true  # Enable planning feature
  planning_llm: true  # Use monica_llm for planning
  
  agents:
    # - mcp_specialist  # 一時的に無効化（ツールの問題）
    - research_agent
    - analysis_agent
    - writer_agent
    - validator_agent
  
  # Manager will receive tasks and delegate appropriately
  tasks:
    - main_task

  # Memory設定（basicメモリとmem0 external memoryを併用）
  memory: true  # basicメモリを有効化
  graph_memory: true  # グラフメモリを有効化
  # external_memoryはcrew_factory.pyで動的に設定される（mem0）
  memory_config:
    provider: "basic"  # 通常のメモリはbasicを使用
    config: {}
    user_memory: {}
  # Mem0設定（crew_factory.pyでexternal_memoryとして設定される）
  mem0_config:
    user_id: "okami_main_crew"  # ユーザーID
    # org_id: "your_org_id"  # オプション
    # project_id: "your_project_id"  # オプション
  cache: true
  verbose: true
  
  knowledge_sources: []

  # Embedder設定（ベクトル化にはollamaを使用）
  embedder:
    provider: "ollama"  # ollamaを使用
    config:
      model: "mxbai-embed-large"
      url: "http://host.docker.internal:11434/api/embeddings"
  
  guardrails:
    - type: "quality"
      min_score: 0.8
    - type: "hallucination"
      threshold: 7.0
  
  delegation_config:
    max_delegation_depth: 3
    allow_cross_delegation: true
    require_approval: false
    smart_routing: true  # Manager uses context to route tasks
  
  monitoring:
    track_delegation: true
    log_decisions: true
    measure_efficiency: true