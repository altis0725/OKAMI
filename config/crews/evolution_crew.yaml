evolution_crew:
  name: "System Evolution Crew"
  description: "A specialized crew for monitoring and evolving the OKAMI system"
  
  process: hierarchical
  
# Use custom manager agent instead of default LLM manager
  manager_agent: manager_agent
  # manager_llm: true

  # Planning configuration
  planning: true  # Enable planning feature
  planning_llm: true  # Use monica_llm for planning

  agents:
    - evolution_agent
    - analysis_agent
    - validator_agent
  
  tasks:
    - evolution_task
  
  # Memory設定（basicメモリとmem0 external memoryを併用）
  memory: true  # basicメモリを有効化
  graph_memory: true  # グラフメモリを有効化
  # external_memoryはcrew_factory.pyで動的に設定される（mem0）
  memory_config:
    provider: "basic"  # 通常のメモリはbasicを使用
    config: {}
    user_memory: {}
  # Mem0設定（crew_factory.pyでexternal_memoryとして設定される）
  mem0_config:
    user_id: "okami_evolution_crew"  # ユーザーID
    # org_id: "your_org_id"  # オプション
    # project_id: "your_project_id"  # オプション
  cache: false  # Always fresh analysis
  verbose: true
  
  # knowledge_sources:
  #   - "knowledge/evolution_strategies.txt"
  #   - "knowledge/performance_baselines.txt"
  knowledge_sources: []
  
  embedder:
    provider: "ollama"
    config:
      model: "mxbai-embed-large"
      url: "http://host.docker.internal:11434/api/embeddings"
  
  evolution_config:
    min_data_points: 100
    confidence_threshold: 0.8
    max_recommendations: 10
    auto_apply: false